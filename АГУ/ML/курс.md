https://www.youtube.com/watch?v=rGG50mZkqQY&t=2019s

- При обучении практически всех моделей (кроме деревьев и их композиций) разные масштабы данных усложнят процесс минимизации функции потерь, так как градиентный спуск с бОльшим трудом будет вычислять градиенты по признакам разного масштаба и потому обновлять веса с разной скоростью, что приведет к медленной сходимости.
- При интерпретации линейных моделей мы можем сравнивать между собой веса только в случае, если признаки имеют одинаковый масштаб.

нужно масштабировать данные!

Исключение составляют деревья и их композиции.

ВЫБРОСЫ ЗЛО!

в данных много выбросов, то после применения StandardScaler могут быть значения, сильно превосходящие все остальные и при этом сильно вылезающие из отрезка [−3;3][−3;3

- Второй способ масштабирования - StandardScaler. При применении этого метода по каждому столбцу вычисляются среднее значение meanmean и стандартное отклонение stdstd, а затем значения столбца преобразуются по формуле

x→x−meanstd.x→stdx−mean​

библиотеке sklearn есть множество различных способов масштабировать данные:

- MinMaxScaler
- MaxAbsScaler
- StandardScaler
- RobustScaler
- QuantileTransformer
- PowerTransformer

_Если нет специфических ограничений на вид признаков после масштабирования, то как правило вид нормализации не важен._


Регуляризация - добавочная функция, целью которой является регулирование другой функции.

Штрафовать означает добавлять в функцию потерь штрафы, то есть некоторые положительные числа:

- чем дальше объект от своего класса внутри разделяющей полосы (или даже снаружи, но не в своем классе) - тем больше штраф ξiξi​ на этом объекте
- если объект попал в свой класс снаружи разделяющей полосы, то он не штрафуется: ξi=0ξi​=0

При обучении SVM в линейно неразделимом случае решаются две задачи:

- максимизируется ширина разделяющей полосы;
- минимизируется сумма штрафов на объектах выборки.

![](loss_func.png)



Объекты, попадающие на границу или внутрь разделяющей полосы, а также объекты, попадающие не в свой класс, называются **_опорными векторами_**.

SVM старается минимизировать количество таких объектов, то есть при обучении ориентируется именно на них.  
Поэтому он и называется _**метод опорных векторов**_

Не рекомендуется использовать accuracy для несбалансированных классов

![](Pasted%20image%2020241208113726.png)

L1 - делает отбор признаков

_L2_ регуляризация (с помощью _L2_-нормы) также называется _Ridge Regression_ (_Гребневая регрессия_) или _регуляризацией по Тихонову_.  
_L1_ регуляризация (с помощью _L1_-нормы) также называется _Lasso Regression_ (_Лассо регрессия_).

$||x||_{p} = (\sum_{i=1}^{n}|x_{i}|)^{1/p}$

на формуле выше - норма Минковского (или расстояние, так и можете ее гуглить). При р = 1, это сумма модулей, при р = 2 привычная нам Евклидова норма, и при р = inf получаем модуль максимального элемента

Задачи оптимизации:
- безусловные
- условные (функция Лагранжа)

Теорма Каруша-Куна-Таккера

![](Pasted%20image%2020241208115148.png)
![](Pasted%20image%2020241208115245.png)![](Pasted%20image%2020241208115549.png)


1) Про L0-регуляризацию на данный момент в курсе мы не рассказывали - именно поэтому она не может быть верным ответом на вопрос. Это один из подходов составления тестов - добавлять в качестве вариантов ответа неизвестные на данном этапе термины - такие ответы заведомо не подойдут. Такая регуляризация на самом деле существует, но в машинном обучении используется крайне редко (в ней мы штрафуем не за величину весов, а за количество ненулевых весов модели).

2) "Каким образом в линейной регрессии появляются полиномы 12 степени?" - я сейчас отвечу максимально верно с терминологической точки зрения, но с практической это не важно (поэтому про это в курсе не говорим). Регрессия называется линейной, если веса входят в формулу в первой степени. В каком виде в формулу входят признаки - будь то степени, будь то функции от признаков - не важно, от этого регрессия не перестает быть ЛИНЕЙНОЙ.

3) "Меня, в частности, интересует вопрос, как сохранить в модели линейной регрессии признаки с существенно различающимся весами на несколько порядков ?" - у обученной в python модели можно обратиться к model.coef_ - там будут лежать ее веса. Если они различаются на несколько порядков - это ничего не меняет ни в каких процессах.
 _L0_ и _L1_ зануляют некоторые веса (_L0_ их однозначно либо зануляет, либо не трогает; а вот _L1_ уменьшает большие, а маленькие зануляет).




Если интересно, почему берём среднее гармоническое (рил вопрос с собеса). Пусть precision = 0, recall = 1. Среднее арифметическое даст 0.5, что довольно неплохой показатель метрики, несмотря на то, что точность нулевая. При среднем гармоническом precision = 0 в числителе занулит метрику такой модели, что более соответствует реальности.

- увеличивая порог, мы увеличиваем precision и уменьшаем _recall_ (так как выдаем кредиты всё меньшему числу клиентов - только тем, в ком уверены)
- уменьшая порог, мы увеличиваем _recall_ и уменьшаем _precision_ (выдаем все больше кредитов - поэтому увеличиваем полноту, но уменьшаем точность).
Величина порога зависит от задач бизнеса. 




## Деревья решений

Гиперпараметров у дерева довольно много. Если посмотреть документацию sklearn, то мы увидим довольно много различных гиперпараметров. Основные из них:

- _criterion_ - критерий информативности, используемый при построении дерева
- _max_depth_ - максимальная глубина дерева
- _min_samples_split_ - минимальное кол
- 
- ичество объектов, которые должны находиться в вершине, чтобы её дальше разбивать
- _min_samples_leaf_ - минимальное количество объектов, которое находится в листе (если после разбиения в одной из полученных подгрупп число объектов меньше, чем min_samples_leaf, то разбиение не производится)
- _max_features_ - число признаков, используемых для поиска наилучшего предиката в каждой вершине.

#### **Стрижка дерева**

рассмотрим _Cost-Complexity pruning_ - стрижку, заключающуюся в добавлении регуляризации к функционалу ошибки дерева.

 Q(T) - функционал ошибки, минимизируемый при построении дерева TT. Добавим регуляризацию:

$$Qα(T)=Q(T)+α∣T∣$$

где ∣T∣∣T∣ - число вершин в дереве TT, αα - коэффициент регуляризации.

Исследования показывают, что использование cost-complexity pruning уменьшает переобучение и при этом сохраняет хорошую предсказательную способность у дерева во многих задачах.
[ссылка](https://stepik.org/lesson/806487/step/3?unit=809663)
У деревьев есть естественное обобщение, которое позволяет обучать деревья и делать предсказания на объектах с пропусками. Разберемся, как это происходит.

У деревьев из sklearn нет такой 

[**Кодирование категориальных признаков**](https://stepik.org/lesson/806487/step/8?unit=809663)


## Бэггинг Случайный лес

Бутсреп - разделение набора данных на случайные выборки с повторяющимися элементами

Бэггинг построение нескольких деревьев
При регрессии выходные переменные усредняются (или другой выбор значения из массива)
При классификации происходит голосования и выбирается самый популярный ответ

Optuna


Лог номарльное распр


После смешивания тяжело интерпритирвовать предикт модели, поэтому редко не применяют на практике


Дописать функции
Сделать шаблон для исследования (через текстовые ячейки)
Прочитать требования