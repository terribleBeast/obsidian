Изучить литру




## Лекция 2

Кластеризация
группировка - значит, присвоить каждому объекты номер кластера.

Каждый объект характеризуется вектором признаков f~1 
$$
(f_1, f_2, ..., f_n) - признаковое\ описание\ объекта\ f
$$
Типы признаков: бинарный, номинальный, упорядоченный (хуйня), количественный

Схожесть объектов определяется через расстояние в многомерном пространстве признаков. 
Функции расстояния Увкледово, косинусовое, Жаккардово, Левенштейна, Хэмминг.

Методы класстеризации
#### Плоская:
* Определяются первоначальные кластеры
* Точки рассматриваются по очерерди и приписываются какому-то кластеру
* Если результат достаточно хорощ, итерации останавливаются 
* В процессе кластеры могут объединяться или распадаться, а точки менять кластер

#### Иерархическая (восходящая и нисходящая)
- Сначала каждая точка - лдин кластер
- На итерации "ближайшие" кластеры объединяются
- Итерации останавливаются при достижении какого-то критерия
- В нисходящей стратегии наоборот: начинаюется с одного большого кластера, включающего все точки

Алгоритм k-средних
1. Выбрать количество кластеров k, которое нам кажется оптимальным для наших данных.
2. Высыпать случайным образом в пространство наших данных k точек (центроидов).
3. Для каждой точки нашего набора данных посчитать, к какому центроиду она ближе.
4. Переместить каждый центроид в центр выборки, которую мы отнесли к этому центроиду.
5. Повторять последние два шага фиксированное число раз, либо до тех пор пока центроиды не "сойдутся" (обычно это значит, что их смещение относительно предыдущего положения не превышает какого-то заранее заданного небольшого значения).


Выбор начальных центроидов:
- Случайный, как можно дальше друг от друга, вуброс может все испортить
- Случайный с выкидыванием отщипенцев
- Предварительная иерархическая клестеризация и центроиды получившихся кластеров первоначальнгых для k-means
- можно проводить над небольшой случайной выборкой из исходных точек
- несколько разных наборов нчальных центроидов и выбор более привлекательного результата.

Выбор k
* Если число кластеров неизвестно априорно, то можно перебрать разные k
* Если поставить цель минимизировать общую внутригрупповую сумму квадратов расстояний центроидов WWS, то лучшее k = N.
* Можно рассмотреть кривую уменьшения WWS в зависимости от увеличения k, и брать k, соответствующий точкам изгиба.
$$
WWS\ = \sum_{i=1}^{k}\sum_{j=1}^{n_i}|x_{ij}-c_i|^2
$$
![](k_means.png)

Характеристика:
* невысокая сложность
* Простое распараллеливание
* Хорошие результаты для "хороших" данных
* Не все данные одинаково хороши

#### Иерархическая кластеризация
Идеи методов:
- агломерации - последовательное объединениии инивидуальных объектов или групп (AGNES, Agglomerative Nesting)
- обратная агломерация (DIANA, Divise Analysis)
Версии агломеративных иерархических процедур отличаются правилами вычисления расстояния между кластерами. 

 алгоритм средней связи (average liinkage clustering) на каждом следующем шае объединяет два ближайших кластера, рассчитывая среднюю арифметическую дистанцию между всеми парами объектов.

Противоположно
алгоритм одиночной связи - "ближайшего соседа" (single linkage clustering), когда расстоянрие между кластерами оценивается как минимальное из дистанций между парами объектов, один из которых кходит в первый кластер, а другой - во второй;

алгоритм полной связи "дальний сосед" (comlete linkage clustering), когда вычисляется расстояние между наболее удаленными объектами

![](иерархическая_модель.png)

Характеристики
- может (иногда) находить кластеры причудливой формы
- N^2 первоначальных сравнений и еще порядка N шагов значит N^3 для большого количества тоек подходит плохо

Другие методы
C-means, K-means++
Кластеризация на основе распределений (EM-алгоритм)
Кластеризация на основе плонтности (DBSCAN): группирует вместе "плотные" участки. Может находить кластеры очень сложной формы.  Проблемы: плостность сильно различна. Трудность в выборе параметров для неизученных датасетов
Графовая и спектральная кластеризация

![](methods_clast.png)









### Полная лекция

**Кластеризация** - разделение набора данных на группы (кластеры) так, чтобы каждый кластер содержал в себе объекты более похожие друг на друга, чем объекты из других кластеров.

**Классификация** - это закономерность, позволяющая делать вывод относительно определения характеристик конкретной группы. Таким образом, *для проведения классификации должны присутствовать признаки*, характеризующие группу, к которой принадлежит то или иное событие или объект.
Различают:
* естественную - производится на основе существенных признаков, характеризующих внутреннюю общность предметов и явлений;
* вспомогательную (искусственную) - производится по внешнему признаку и служит для придания множеству объектов нужного порядка.

В зависимости от выбранных признаков, их сочетания и процедуры деления понятий классификация может быть:
- простой - деление понятия только по одному признаку и только один раз до раскрытия всех видов. Пример: дихотомия



## Метрики

ARI - показывает на сколько кластеризация случайна 



Ресурсы:
1) [ИНТУИТ](https://intuit.ru/studies/courses/6/6/lecture/166?page=1)
2) [курс](https://habr.com/ru/companies/ods/articles/325654/)
3) https://www.dmitrymakarov.ru/intro/clustering-16/